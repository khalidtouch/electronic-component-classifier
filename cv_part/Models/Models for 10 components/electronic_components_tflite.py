# -*- coding: utf-8 -*-
"""Electronic_Components_TFLite.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zzEhOayDtwLHiEB0EVcNBukBS4wKQuJL

<font size="+3"><strong>Development of a Mobile Application and Computer Vision Model for Automated Identification of Electronics Components</strong></font>

# Prepare Data

## Import
"""

# Importing necessary libraries
import warnings

import tensorflow as tf
# assert tf.__version__.startswith('2')

from google.colab import files # For downloading Files
from google.colab import drive # For Mounting Google drive
import os
import numpy as np
import matplotlib.pyplot as plt
import pathlib
from tensorflow.keras import regularizers


from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.metrics import precision_score, recall_score, f1_score

warnings.simplefilter(action="ignore", category=FutureWarning)

# Mounting Google Drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Moving to the necessary folder

# %cd drive/MyDrive/Development of a Mobile Application and Computer Vision Model for Automated Identification of Electronics Components/New Dataset

# Commented out IPython magic to ensure Python compatibility.
# %ls -la

"""## Split"""

# Setting the base directory and defines constants for the data generator and model training

base_dir = 'upload'
base_dir = pathlib.Path(base_dir)
test_dir = 'Test_Datasets'
test_dir = pathlib.Path(test_dir)
VALIDATION_SPLIT = 0.2
SEED = 100
BATCH_SIZE = 32
IMAGE_SIZE = 128
# IMAGE_SIZE = 64
base_learning_rate = 0.0001

# Creating the Image data generator with data augmentation

datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255,
    validation_split=VALIDATION_SPLIT
    )

train_generator = datagen.flow_from_directory(
    base_dir,
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    # shuffle=True,
    seed=SEED,
    subset='training')

val_generator = datagen.flow_from_directory(
    base_dir,
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    # shuffle=True,
    seed=SEED,
    subset='validation')


test_generator = datagen.flow_from_directory(
    test_dir,  # Path to the directory containing your testing images
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    seed=SEED,
    shuffle=False  # Set shuffle to False for testing set
)

num_classes=10

for image_batch, label_batch in train_generator:
    break
image_batch.shape, label_batch.shape

# Generator is used to retrieve batch of images and labels for training

print (train_generator.class_indices)

labels = '\n'.join(sorted(train_generator.class_indices.keys()))

with open('labels.txt', 'w') as f:
    f.write(labels)

!cat labels.txt

"""## Explore"""

# Display a few sample images
fig, axes = plt.subplots(2, 4, figsize=(12, 6))
axes = axes.flatten()

for ax, (image, label) in zip(axes, train_generator):
    ax.imshow(image[0])
    ax.set_title(label[0])
    ax.axis('off')

plt.tight_layout()
plt.show()

"""Examine RGB values in an Image matrix

"""

# Accessing the first image in the batch
image = image_batch[0]

# Splitting the image into RGB channels
red_channel = image[:, :, 0]
green_channel = image[:, :, 1]
blue_channel = image[:, :, 2]

# Plotting the RGB channels
plt.figure(figsize=(10, 4))

plt.subplot(1, 4, 1)
plt.imshow(image)
plt.title('Original Image')
plt.axis('off')

plt.subplot(1, 4, 2)
plt.imshow(red_channel, cmap='gray')
plt.title('Red Channel')
plt.axis('off')

plt.subplot(1, 4, 3)
plt.imshow(green_channel, cmap='gray')
plt.title('Green Channel')
plt.axis('off')

plt.subplot(1, 4, 4)
plt.imshow(blue_channel, cmap='gray')
plt.title('Blue Channel')
plt.axis('off')

plt.tight_layout()
plt.show()

# Accessing the first image in the batch
sample_image = train_generator[0][0][0]  # First image in the first batch

# Getting the size of the sample image
image_height, image_width, _ = sample_image.shape

# Printing the size
print(f"Sample image size: {image_width} x {image_height}")

"""# Build Model

<p>A convolutional neural network (CNN). CNNs are a specific kind of artificial neural network that is very effective for image classification because they are able to take into account the spatial coherence of the image, i.e., that pixels close to each other are often related.</p>

<p>Building a CNN begins with specifying the model type. In our case, we'll use a <a href="https://keras.io/getting-started/sequential-model-guide/">Sequential</a> model, which is a linear stack of layers. We'll then add on it.</p>

A complete neural network architecture will have a number of other layers that are designed to play a specific role in the overall functioning of the network. Much deep learning research is about how to structure these layers into coherent systems.</p>
<p>layers:</p>
<ul>

- `tf.keras.Sequential`: This is the base model class in Keras that allows you to stack layers sequentially.

- `tf.keras.Input(shape=IMG_SHAPE)`: This creates an input layer with the specified shape (`IMG_SHAPE`). The input shape represents the shape of the input images that will be fed into the model.

- `tf.keras.layers.experimental.preprocessing.RandomZoom(0.2)`: This layer randomly applies zoom augmentation to the input images. It randomly zooms in or out of the images by a factor of 0.2.

- `tf.keras.layers.experimental.preprocessing.RandomTranslation(0.2, 0.2)`: This layer randomly applies translation augmentation to the input images. It randomly shifts the images horizontally and vertically by a maximum of 0.2.

- `base_model`: This is a reference to a pre-trained model that will be used as a base for feature extraction. You need to replace `base_model` with an actual pre-trained model, such as ResNet50 or InceptionV3.

- `tf.keras.layers.GlobalAveragePooling2D()`: This layer performs global average pooling on the output of the base model. It reduces the spatial dimensions of the feature maps to a fixed size, regardless of the input image size.

- `tf.keras.layers.Dense(512, activation='relu')`: This adds a fully connected dense layer with 512 units and ReLU activation function. It serves as a hidden layer to learn complex representations from the pooled features.

- `tf.keras.layers.BatchNormalization()`: This layer normalizes the activations of the previous layer by adjusting and scaling them. It helps in stabilizing the learning process and improving generalization.

- `tf.keras.layers.Dropout(0.5)`: This layer applies dropout regularization to the inputs. It randomly sets a fraction of input units to 0 at each update during training, which helps in reducing overfitting.

- `tf.keras.layers.Dense(num_classes, activation='softmax')`: This adds the final dense layer with `num_classes` units (representing the number of classes in your classification problem) and softmax activation function. It outputs the predicted probabilities for each class.

Each layer in the model contributes to the overall architecture and helps in learning useful representations from the input images. The combination of data augmentation, base model, pooling, and fully connected layers helps in capturing and extracting meaningful features for classification tasks.
</ul>
<p>To take a look at how it all stacks up, we'll print the model summary. Notice that our model has a whopping <code>3,669,249</code> paramaters. These are the different weights that the model learns through training and what are used to generate predictions on a new image.</p>
<p><img src="https://assets.datacamp.com/production/project_555/img/mlp_conv.png" alt></p>

## Baseline Model / Models
"""

# MobileNetV2 base model

IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)

# Experiementing with ResNet50
# base_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=IMG_SHAPE)

# Create the base model from the pre-trained model MobileNet V2
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                              include_top=False,
                                              weights='imagenet')

base_model.trainable = False

data_augmentation = tf.keras.Sequential([
    tf.keras.layers.experimental.preprocessing.RandomZoom(0.5),
    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),
    tf.keras.layers.experimental.preprocessing.RandomFlip("horizontal"),
    tf.keras.layers.experimental.preprocessing.RandomContrast(0.2),
    tf.keras.layers.experimental.preprocessing.RandomTranslation(0.2, 0.2),
])

# First Model I tested

# model = tf.keras.Sequential([
#     tf.keras.Input(shape=IMG_SHAPE),
#     tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),
#     tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),
#     base_model,
#     #tf.keras.layers.Conv2D(64, 3, activation='relu'),
#     tf.keras.layers.GlobalAveragePooling2D(),
#     tf.keras.layers.Dropout(0.2),
#     tf.keras.layers.Dense(num_classes, activation='softmax')
# ])

#Experimented with this Model

# model = tf.keras.Sequential([
#     tf.keras.Input(shape=IMG_SHAPE),
#     tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),
#     tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),
#     tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),
#     tf.keras.layers.experimental.preprocessing.RandomTranslation(0.2, 0.2),
#     base_model,
#     tf.keras.layers.GlobalAveragePooling2D(),
#     tf.keras.layers.BatchNormalization(),  # Added batch normalization
#     tf.keras.layers.Dropout(0.5),  # Increased dropout rate
#     tf.keras.layers.Dense(512, activation='relu'),  # Added a dense layer
#     tf.keras.layers.BatchNormalization(),  # Added batch normalization
#     tf.keras.layers.Dropout(0.5),  # Increased dropout rate
#     tf.keras.layers.Dense(num_classes, activation='softmax')
# ])

# # Experimented with this too

# model = tf.keras.Sequential([
#     tf.keras.Input(shape=IMG_SHAPE),
#     tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),
#     tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),
#     base_model,
#     tf.keras.layers.GlobalAveragePooling2D(),
#     tf.keras.layers.Dropout(0.2),
#     tf.keras.layers.Conv2D(64, 3, activation='relu'),
#     tf.keras.layers.GlobalAveragePooling2D(),
#     tf.keras.layers.Dropout(0.2),
#     tf.keras.layers.Dense(num_classes, activation='softmax')
# ])

# Tried This too

# model = tf.keras.Sequential([
#     tf.keras.Input(shape=IMG_SHAPE),
#     tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),
#     tf.keras.layers.experimental.preprocessing.RandomTranslation(0.2, 0.2),
#     base_model,
#     tf.keras.layers.Conv2D(32, 3, activation='relu'),
#     tf.keras.layers.GlobalAveragePooling2D(),
#     tf.keras.layers.BatchNormalization(),  # Added batch normalization
#     tf.keras.layers.Dropout(0.5),  # Increased dropout rate
#     tf.keras.layers.Dense(512, activation='relu'),  # Added a dense layer
#     tf.keras.layers.BatchNormalization(),  # Added batch normalization
#     tf.keras.layers.Dropout(0.5),  # Increased dropout rate
#     tf.keras.layers.Dense(num_classes, activation='softmax')
# ])

# Final Architecture.

model = tf.keras.Sequential([
    tf.keras.Input(shape=IMG_SHAPE),
    data_augmentation,
    base_model,
    tf.keras.layers.Conv2D(64, 3, activation='relu'),
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

# from tensorflow.keras import regularizers

# model = tf.keras.Sequential([
#     tf.keras.Input(shape=IMG_SHAPE),
#     tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),
#     tf.keras.layers.experimental.preprocessing.RandomZoom(0.5),
#     tf.keras.layers.experimental.preprocessing.RandomTranslation(0.2, 0.2),
#     base_model,
#     tf.keras.layers.Conv2D(64, 3, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
#     tf.keras.layers.GlobalAveragePooling2D(),
#     tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
#     tf.keras.layers.BatchNormalization(),
#     tf.keras.layers.Dropout(0.5),
#     tf.keras.layers.Dense(num_classes, activation='softmax')
# ])

"""## Iterate and Evaluate"""

model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

print('Number of trainable variables = {}'.format(len(model.trainable_variables)))

loss0, accuracy0 = model.evaluate(val_generator)

es = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy',
    # min_delta=0,
    patience=5,
    verbose=1,
    mode='max')

# checkpoint_filepath = 'path/best_model'

# # Create a ModelCheckpoint callback to save the best model
# mcc = tf.keras.callbacks.ModelCheckpoint(
#     filepath=checkpoint_filepath,
#     monitor='val_accuracy',
#     save_best_only=True,
#     mode='max',
#     verbose=1
# )

"""More iterations"""

initial_epochs = 100

history = model.fit(train_generator,
                    steps_per_epoch=len(train_generator),
                    epochs=initial_epochs,
                    validation_data=val_generator,
                    callbacks=[es],
                    validation_steps=len(val_generator))

"""Accuracy and Loss

"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']



loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,2.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

# Assuming you have already trained your model

# Evaluate model on the testing set
test_loss, test_accuracy = model.evaluate(test_generator)

print(f'Test Loss: {test_loss:.2f}')
print(f'Test Accuracy: {test_accuracy:.2f}')

"""Precision, Recall, F1"""

# Make predictions on the validation data
y_true = test_generator.classes
y_pred = model.predict(test_generator)
y_pred_labels = np.argmax(y_pred, axis=1)  # Convert predicted probabilities to class labels

# Calculate Precision
precision = precision_score(y_true, y_pred_labels, average='macro')

# Calculate Recall
recall = recall_score(y_true, y_pred_labels, average='macro')

# Calculate F1 Score
f1 = f1_score(y_true, y_pred_labels, average='macro')

# Print the results
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')

"""Confusion Matrix"""

# Step 1: Make predictions on the validation data
y_true = test_generator.classes
y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(y_pred, axis=1)

# Step 2: Calculate the confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Step 3: Plot the confusion matrix
plt.figure(figsize=(8, 8))
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d", cbar=False)
plt.xlabel("Predicted Class")
plt.ylabel("True Class")
plt.title("Confusion Matrix")
plt.show()

"""ROC Curve + AUC"""

# Step 1: Make predictions on the validation data
y_pred_prob = model.predict(test_generator)
y_true = test_generator.classes
num_classes = test_generator.num_classes

# Step 2: Convert probabilities to binary matrix
y_pred_bin = np.argmax(y_pred_prob, axis=1)
y_true_bin = label_binarize(y_true, classes=np.arange(num_classes))

# Step 3: Compute ROC curve and AUC for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Step 4: Compute macro-average ROC curve and AUC
fpr_macro = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))
tpr_macro = np.zeros_like(fpr_macro)
for i in range(num_classes):
    tpr_macro += np.interp(fpr_macro, fpr[i], tpr[i])
tpr_macro /= num_classes
roc_auc_macro = auc(fpr_macro, tpr_macro)

# Step 5: Plot ROC curves for each class
plt.figure(figsize=(8, 8))
for i in range(num_classes):
    plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')
plt.plot(fpr_macro, tpr_macro, label=f'Macro Average (AUC = {roc_auc_macro:.2f})', linestyle='--')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')

plt.show()

# Iterate

epochs = 100

history1 = model.fit(train_generator,
                    steps_per_epoch=len(train_generator),
                    epochs=epochs,
                    validation_data=val_generator,
                    initial_epoch=history.epoch[-1],
                    callbacks=[es],
                    validation_steps=len(val_generator))

"""Accuracy and Loss # second iteration


"""

acc = history1.history['accuracy']
val_acc = history1.history['val_accuracy']



loss = history1.history['loss']
val_loss = history1.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,2.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

# Assuming you have already trained your model

# Evaluate model on the testing set
test_loss, test_accuracy = model.evaluate(test_generator)

print(f'Test Loss: {test_loss:.2f}')
print(f'Test Accuracy: {test_accuracy:.2f}')

"""Precision, Recall, F1 # second iteration"""

# Make predictions on the validation data
y_true = test_generator.classes
y_pred = model.predict(test_generator)
y_pred_labels = np.argmax(y_pred, axis=1)  # Convert predicted probabilities to class labels

# Calculate Precision
precision = precision_score(y_true, y_pred_labels, average='macro')

# Calculate Recall
recall = recall_score(y_true, y_pred_labels, average='macro')

# Calculate F1 Score
f1 = f1_score(y_true, y_pred_labels, average='macro')

# Print the results
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')

"""Confusion Matrix # second iteration"""

# Step 1: Make predictions on the validation data
y_true = test_generator.classes
y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(y_pred, axis=1)

# Step 2: Calculate the confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Step 3: Plot the confusion matrix
plt.figure(figsize=(8, 8))
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d", cbar=False)
plt.xlabel("Predicted Class")
plt.ylabel("True Class")
plt.title("Confusion Matrix")
plt.show()

"""ROC Curve + AUC # second iteration"""

# Step 1: Make predictions on the validation data
y_pred_prob = model.predict(test_generator)
y_true = test_generator.classes
num_classes = test_generator.num_classes

# Step 2: Convert probabilities to binary matrix
y_pred_bin = np.argmax(y_pred_prob, axis=1)
y_true_bin = label_binarize(y_true, classes=np.arange(num_classes))

# Step 3: Compute ROC curve and AUC for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Step 4: Compute macro-average ROC curve and AUC
fpr_macro = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))
tpr_macro = np.zeros_like(fpr_macro)
for i in range(num_classes):
    tpr_macro += np.interp(fpr_macro, fpr[i], tpr[i])
tpr_macro /= num_classes
roc_auc_macro = auc(fpr_macro, tpr_macro)

# Step 5: Plot ROC curves for each class
plt.figure(figsize=(8, 8))
for i in range(num_classes):
    plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')
plt.plot(fpr_macro, tpr_macro, label=f'Macro Average (AUC = {roc_auc_macro:.2f})', linestyle='--')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')

plt.show()

"""### Fine Tuning / Iterate and Evaluate


"""

# # Set base_model.trainable = True to enable fine-tuning
# base_model.trainable = True

# # Compile the model with an appropriate optimizer and loss function
# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),
#               loss=tf.keras.losses.CategoricalCrossentropy(),
#               metrics=['accuracy'])

# # Print a summary of the model
# model.summary()

# tf.keras.layers.Dropout(0.6),

base_model.trainable = True

# Let's take a look to see how many layers are in the base model
print("Number of layers in the base model: ", len(base_model.layers))

# Fine tune from this layer onwards
fine_tune_at = 100

# Freeze all the layers before the `fine_tune_at` layer
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable =  False

print(base_learning_rate)

model.compile(loss='categorical_crossentropy',
              optimizer = tf.keras.optimizers.Adam(lr=base_learning_rate/10),
              metrics=['accuracy'])

model.summary()

print('Number of trainable variables = {}'.format(len(model.trainable_variables)))

fine_tune_epochs = 25
total_epochs =  initial_epochs + fine_tune_epochs
history_fine = model.fit(train_generator,
                         steps_per_epoch=len(train_generator),
                         epochs=total_epochs,
                         initial_epoch=history.epoch[-1],
                         callbacks=[es],
                         validation_data=val_generator,
                         validation_steps=len(val_generator))

"""Accuracy and Loss # second iteration


"""

acc = history_fine.history['accuracy']
val_acc = history_fine.history['val_accuracy']



loss = history_fine.history['loss']
val_loss = history_fine.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,2.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

# Assuming you have already trained your model

# Evaluate model on the testing set
test_loss, test_accuracy = model.evaluate(test_generator)

print(f'Test Loss: {test_loss:.2f}')
print(f'Test Accuracy: {test_accuracy:.2f}')

"""Precision, Recall, F1 # second iteration"""

# Make predictions on the validation data
y_true = test_generator.classes
y_pred = model.predict(test_generator)
y_pred_labels = np.argmax(y_pred, axis=1)  # Convert predicted probabilities to class labels

# Calculate Precision
precision = precision_score(y_true, y_pred_labels, average='macro')

# Calculate Recall
recall = recall_score(y_true, y_pred_labels, average='macro')

# Calculate F1 Score
f1 = f1_score(y_true, y_pred_labels, average='macro')

# Print the results
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')

"""Confusion Matrix # second iteration"""

# Step 1: Make predictions on the validation data
y_true = test_generator.classes
y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(y_pred, axis=1)

# Step 2: Calculate the confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)

# Step 3: Plot the confusion matrix
plt.figure(figsize=(8, 8))
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d", cbar=False)
plt.xlabel("Predicted Class")
plt.ylabel("True Class")
plt.title("Confusion Matrix")
plt.show()

"""ROC Curve + AUC # second iteration"""

# Step 1: Make predictions on the validation data
y_pred_prob = model.predict(test_generator)
y_true = test_generator.classes
num_classes = test_generator.num_classes

# Step 2: Convert probabilities to binary matrix
y_pred_bin = np.argmax(y_pred_prob, axis=1)
y_true_bin = label_binarize(y_true, classes=np.arange(num_classes))

# Step 3: Compute ROC curve and AUC for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Step 4: Compute macro-average ROC curve and AUC
fpr_macro = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))
tpr_macro = np.zeros_like(fpr_macro)
for i in range(num_classes):
    tpr_macro += np.interp(fpr_macro, fpr[i], tpr[i])
tpr_macro /= num_classes
roc_auc_macro = auc(fpr_macro, tpr_macro)

# Step 5: Plot ROC curves for each class
plt.figure(figsize=(8, 8))
for i in range(num_classes):
    plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')
plt.plot(fpr_macro, tpr_macro, label=f'Macro Average (AUC = {roc_auc_macro:.2f})', linestyle='--')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')

plt.show()

"""# Saving Models"""

# Commented out IPython magic to ensure Python compatibility.
# %pwd

# Naive Model

saved_model_dir = 'save/naive_model'
tf.saved_model.save(model, saved_model_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()

with open('mobilenet_v2_naive_10model.tflite', 'wb') as f:
    f.write(tflite_model)

files.download('mobilenet_v2_naive_10model.tflite')
files.download('labels.txt')

# Fine Tuned Model

saved_model_dir = 'save/fine_tuning'
tf.saved_model.save(model, saved_model_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()

with open('mobilenet_v2_fine_tuned10.tflite', 'wb') as f:
    f.write(tflite_model)

files.download('mobilenet_v2_fine_tuned10.tflite')
files.download('labels.txt')